---
layout: single
title: "특성 공학과 규제"
categories: ML
tag: [AI, ML, Python, 특성공학과규제]
toc: true # 목차 설정
---

https://ml-ko.kr/

# 다중 회귀(multiple regression)

- 여러 개의 특성을 사용한 선형 회귀
  - 따라서 다중 회귀 모델을 훈련하는 것은 선형 회귀 모델을 훈련 하는 것과 같음.
  - 다만 여러 개의 특성을 사용하여 선형 회귀를 수행함.

# 특성 공학(feature engineering)

- 기존의 특성을 사용해 새로운 특성을 뽑아내는 작업

### 데이터 준비

```python
import pandas as pd
df  = pd.read_csv("https://bit.ly/perch_csv_data")
perch_full = df.to_numpy()
print(perch_full) # 길이 높이 두께 3개의 특성
```

    [[ 8.4   2.11  1.41]
     [13.7   3.53  2.  ]
     [15.    3.82  2.43]
     [16.2   4.59  2.63]
     [17.4   4.59  2.94]
     [18.    5.22  3.32]
     [18.7   5.2   3.12]
     [19.    5.64  3.05]
     [19.6   5.14  3.04]
     [20.    5.08  2.77]
     [21.    5.69  3.56]
     [21.    5.92  3.31]
     [21.    5.69  3.67]
     [21.3   6.38  3.53]
     [22.    6.11  3.41]
     [22.    5.64  3.52]
     [22.    6.11  3.52]
     [22.    5.88  3.52]
     [22.    5.52  4.  ]
     [22.5   5.86  3.62]
     [22.5   6.79  3.62]
     [22.7   5.95  3.63]
     [23.    5.22  3.63]
     [23.5   6.28  3.72]
     [24.    7.29  3.72]
     [24.    6.38  3.82]
     [24.6   6.73  4.17]
     [25.    6.44  3.68]
     [25.6   6.56  4.24]
     [26.5   7.17  4.14]
     [27.3   8.32  5.14]
     [27.5   7.17  4.34]
     [27.5   7.05  4.34]
     [27.5   7.28  4.57]
     [28.    7.82  4.2 ]
     [28.7   7.59  4.64]
     [30.    7.62  4.77]
     [32.8  10.03  6.02]
     [34.5  10.26  6.39]
     [35.   11.49  7.8 ]
     [36.5  10.88  6.86]
     [36.   10.61  6.74]
     [37.   10.84  6.26]
     [37.   10.57  6.37]
     [39.   11.14  7.49]
     [39.   11.14  6.  ]
     [39.   12.43  7.35]
     [40.   11.93  7.11]
     [40.   11.73  7.22]
     [40.   12.38  7.46]
     [40.   11.14  6.63]
     [42.   12.8   6.87]
     [43.   11.93  7.28]
     [43.   12.51  7.42]
     [43.5  12.6   8.14]
     [44.   12.49  7.6 ]]

```python
import numpy as np
perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])
```

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state=42)
```

사이킷런은 **특성을 만들거나 전처리 하기 위한** 다양한 클래스를 제공한다.  
이런 클래스를 **변환기(transformer)**라고 부른다.  
사이킷런의 모델 클래스에 일관된 fit(), score(), predict() 메서드가 있는 것처럼  
변환기 클래스는 fit(), transform() 메서드를 제공함.  
LinearRegression 같은 모델 클래스는 추정기(estimator)라고도 부른다.

```python
from sklearn.preprocessing import PolynomialFeatures # 변환기
```

### PolynomialFeatures의 예시

```python
poly = PolynomialFeatures()
poly.fit([[2,3]]) # 새롭게 만들 특성 조합을 찾음
print(poly.transform([[2,3]])) # 실제 데이터로 변환
```

    [[1. 2. 3. 4. 6. 9.]]

PolynomialFeatures 클래스는 기본적으로 특성을 제곱한 항을 추가하고 특성끼리 서로 곱한 합을 추가함.

- interaction_only가 True이면 거듭제곱 항은 제외되고 특성 간의 곱셈 항만 추가

`[[1. 2. 3. 4. 6. 9.]]`에서 1은 왜 추가된 것일까?  
무게 = a _ 길이 + b _ 높이 + c _ 두께 + d _ 1  
이유 : 선형 방정식의 절편은 항상 값이 1인 특성과 곱해주는 계수라고 생각해 볼 수 있음.  
그러나 사이킷런 선형 모델은 자동으로 절편을 추가하므로 굳이 이렇게 특성을 만들 필요 없음.  
include_bias=False로 지정하면 1이 추가되지 않음.

```python
poly = PolynomialFeatures(include_bias=False)
poly.fit([[2,3]])
print(poly.transform([[2,3]]))
```

    [[2. 3. 4. 6. 9.]]

### 훈련 세트, 테스트 세트에 PolynomialFeatures 사용

```python
poly = PolynomialFeatures(include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
print(train_poly.shape)
```

    (42, 9)

```python
poly.get_feature_names() # 특성이 각각 어떤 입력의 조합으로 만들어졌는지 보여줌
```

    /usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
      warnings.warn(msg, category=FutureWarning)





    ['x0', 'x1', 'x2', 'x0^2', 'x0 x1', 'x0 x2', 'x1^2', 'x1 x2', 'x2^2']

```python
test_poly = poly.transform(test_input)
```

### 다중 회귀 모델 훈련

```python
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))
print(lr.score(test_poly, test_target))
# 과대적합이라 볼 수 있음.
```

    0.9903183436982124
    0.9714559911594134

PolynomialFeatures 클래스의 degree 매개변수를 사용하면 최고차항의 차수를 지정할 수 있음.

```python
poly = PolynomialFeatures(degree=5, include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)
print(train_poly.shape) # 특성의 개수가 55
```

    (42, 55)

```python
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))
print(lr.score(test_poly, test_target))
```

    0.9999999999991097
    -144.40579242684848

특성의 개수를 크게 늘리면 선형 모델은 아주 강력해진다.  
훈련 세트에 대해 거의 완벽하게 학습할 수 있기 때문.  
하지만 너무 과대적합되므로 테스트 세트에서는 점수가 좋지 못함.

샘플 개수보다 특성이 더 많다면?  
위에서 사용한 훈련 세트의 샘플 개수는 42개, 특성은 55개이다.  
42개의 샘플을 55개의 특성으로 훈련하면 완벽 하게 학습할 수 있는 것이 당연함.  
예를 들어 42개의 참새를 맞추기 위해 딱 한 번 새총을 쏴야 한다면 참새 떼 중앙을 겨냥하여 가능한 한 맞출 가능성을 높여야 하지만,  
55번이나 쏠 수 있다면 한 번에 하나씩 모든 참새를 맞출 수 있는 꼴임.

# 규제(regularization)

- 머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 방해하는 것
- 즉 모델이 훈련 세트에 과대적합되지 않도록 만드는 것
  - 선형 회귀 모델의 경우 특성에 곱해지는 계수(또는 기울기)의 크기를 작게 만드는 일

규제하기 전에 특성의 스케일에 대해 고려해야함.  
왜냐하면 특성의 스케일이 정규화되지 않으면 여기에 곱해지는 계수 값도 차이가 나게 됨.  
일반적으로 선형 회귀 모델에 규제를 적용할때 계수 값의 크기가 서로 많이 다르면 공정하게 제어될 수 없음.  
따라서 규제를 적용하기 전에 정규화(표준점수 등)를 해야함!

그럼 지금까지 선형 회귀 문제에서 정규화를 안한건?  
사이킷런의 LinearRegression은 특성의 스케일에 영향을 받지 않는 알고리즘으로 구현되어 있음.

```python
from sklearn.preprocessing import StandardScaler # 표준점수로 정규화하는 클래스, 변환기

ss = StandardScaler()
# 훈련 세트에서 학습한 평균과 표준편차는 mean_, scale_ 속성에 저장됨.
ss.fit(train_poly)
# 꼭 훈련 세트로 학습한 변환기를 사용해 테스트 세트까지 변환해야 됨.
# 왜냐하면 훈련 세트를 기준으로 데이터를 학습시키고 테스트하기 때문.
train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)
```

### 선형 회귀 모델에 규제를 추가한 모델

- 릿지(ridge)
  - 계수를 제곱한 값을 기준으로 규제를 적용
  - solver 매개변수에 최적의 모델을 찾기 위한 방법을 지정할 수 있음. 기본값 = 'auto'
- 라쏘(lasso)
  - 계수의 절댓값을 기준으로 규제를 적용
  - 최적의 모델을 찾귀 위해 좌표축을 따라 최적화를 수행해가는 좌표 하강법을 사용함.
- 일반 적으로 릿지를 좀 더 선호함
  - 왜냐하면 두 알고리즘 모두 계수의 크기를 줄이지만 라쏘는 아예 0으로 만들 수도 있음.

```python
# 릿지 회귀
from sklearn.linear_model import Ridge

ridge = Ridge()
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))
print(ridge.score(train_scaled, train_target))
```

    0.9896101671037343
    0.9896101671037343

릿지와 라쏘 모델을 사용할 때 규제의 양을 임의로 조절할 수 있음.  
모델 객체를 만들 때 alpha 매개변수로 규제의 강도를 조절함.  
alpha값이 크면 규제 강도가 강해지므로 계수 값이 더 줄고 조금 더 과소적합되도록 유도함.  
alpha값이 작으면 계수를 줄이는 역할이 줄고 선형 회귀 모델과 유사해지므로 과대적합될 가능성이 큼.

### 하이퍼파라미터(hyperparameter)

- 머신러닝 모델이 학습할 수 없고 사람이 알려줘야 하는 파라미터
- 규제의 양을 조절하는 alpha 매개변수가 하이퍼파라미터임.

### 적절한 alpha 값을 찾는 방법

- alpha 값에 대한 R^2 값의 그래프를 그려 보는 것
- 훈련 세트와 테스트 세트의 점수가 가장 가까운 지점이 최적의 값

```python
import matplotlib.pyplot as plt
train_score = []
test_score = []
# 기본적으로 10의 배수(상용로그 스케일)로 하이퍼파라미터 범위를 지정하는 것이 관례임.
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
    ridge = Ridge(alpha=alpha)
    ridge.fit(train_scaled, train_target)
    train_score.append(ridge.score(train_scaled, train_target))
    test_score.append(ridge.score(test_scaled, test_target))
```

```python
# 간격을 맞춰주기 위해 상용로그를 사용하여 지수로 바꿔줌.
plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()
```

![png]({{site.url}}/../../assets/images/2023-04-14-fe/output_34_0.png)

```python
ridge = Ridge(alpha=0.1) # 최적의 alpha값
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))
```

    0.9903815817570365
    0.9827976465386884

```python
# 라쏘 회귀
from sklearn.linear_model import Lasso
lasso = Lasso()
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))
```

    0.989789897208096
    0.9800593698421883

```python
train_score = []
test_score = []
# 기본적으로 10의 배수(상용로그 스케일)로 하이퍼파라미터 범위를 지정하는 것이 관례임.
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
    # 라쏘 모델은 최적의 계수를 찾기 위해 반복 연산을 수행하는데, 지정한 횟수가 부족할 때 뜨는 경고를 막고자 매개변수를 지정함.
    lasso = Lasso(alpha=alpha, max_iter=10000)
    lasso.fit(train_scaled, train_target)
    train_score.append(lasso.score(train_scaled, train_target))
    test_score.append(lasso.score(test_scaled, test_target))
```

    /usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.878e+04, tolerance: 5.183e+02
      model = cd_fast.enet_coordinate_descent(
    /usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.297e+04, tolerance: 5.183e+02
      model = cd_fast.enet_coordinate_descent(

```python
plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()
```

![png]({{site.url}}/../../assets/images/2023-04-14-fe/output_38_0.png)

```python
lasso = Lasso(alpha=10) # 최적의 alpha값
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))
```

    0.9888067471131867
    0.9824470598706695

```python
# 라쏘 회귀 모델에서 계수가 0이 된 특성이 40개임.
print(np.sum(lasso.coef_ == 0)) # 브로드캐스팅을 통해 True=1, False=0 으로 원소를 바꿈.
# 이런 특징 때문에 라쏘 모델을 유용한 특성(의미있는 특성)을 골라내는 용도로 사용할 수 있음.
```

    40
